{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a target=\"_blank\" href=\"https://colab.research.google.com/github/amjadraza/datafy-llm-workshop/blob/main/notebooks/01_LiteLLM_Models.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"hylINLusnqJF"},"source":["# Hands-on: LLM using LiteLLLM Package\n","\n","https://docs.litellm.ai/\n","\n","**LiteLLM a light package to simplify calling OpenAI, Azure, Cohere, Anthropic, Huggingface API Endpoints.** It manages:\n","\n","- translating inputs to the provider's completion and embedding endpoints\n","- guarantees consistent output, text responses will always be available at `['choices'][0]['message']['content']`\n","- exception mapping - common exceptions across providers are mapped to the OpenAI exception types"]},{"cell_type":"markdown","metadata":{"id":"wiqc8OFJIn6m"},"source":["Standard Output Object: Using OpenAI API Specifications. For more details read on https://platform.openai.com/docs/api-reference/chat/object\n","\n","```\n","<OpenAIObject chat.completion id=chatcmpl-81WDml2zvKjlA6Vm8QMsNK8hfEVPU at 0x7a09a3cb9080> JSON: {\n","  \"id\": \"chatcmpl-81WDml2zvKjlA6Vm8QMsNK8hfEVPU\",\n","  \"object\": \"chat.completion\",\n","  \"created\": 1695372878,\n","  \"model\": \"gpt-3.5-turbo-0613\",\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"Hello! I am an AI language model, so I don't have feelings, but I'm here to help you. How can I assist you today?\"\n","      },\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"usage\": {\n","    \"prompt_tokens\": 13,\n","    \"completion_tokens\": 31,\n","    \"total_tokens\": 44\n","  },\n","  \"response_ms\": 2123.604\n","}\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"54J8rFNVZ9dJ"},"source":["# 🔑 LiteLLM Keys (Access Claude-2, Llama2-70b, etc.)\n","\n","Using LiteLLM Keys, user can try some models free. List of models to be tried can be found on below link.\n","\n","https://docs.litellm.ai/docs/proxy_api#supported-models-for-litellm-key\n","\n","> LiteLLM provides a free $10 community-key for testing all providers on LiteLLM. You can replace this with your own key. Send email to krrish@berri.ai for dedicated key."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8144,"status":"ok","timestamp":1695948579543,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"saNrVbfnmTuY","outputId":"b25869ab-56e3-440b-949b-37bc16c74851"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install litellm -qU"]},{"cell_type":"markdown","metadata":{"id":"uM6mQdmjCvqx"},"source":["#OpenAI\n","\n","LiteLLM Supports OpenAI API Models. To read details about the support Models by OpenAI API follow https://docs.litellm.ai/docs/providers/openai.\n","\n","\n","> liteLLM provides a free $10 community-key for testing all providers on LiteLLM. You can replace this with your own key. Send email to krrish@berri.ai for dedicated key."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhMogO5MDCPg"},"outputs":[],"source":["import os\n","from litellm import completion\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"sk-litellm-7_NPZhMGxY2GoHC59LgbDw\" # [OPTIONAL] replace with your openai key\n","\n","\n","messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n","\n","# openai call\n","# response = completion(\"gpt-3.5-turbo\", messages)\n","\n","response = completion(\"gpt-4\", messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1695377589165,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"RTGsRcTaDTEf","outputId":"00e636a7-c14f-46ec-8e0d-1a2082af62c0"},"outputs":[{"data":{"text/plain":["<OpenAIObject chat.completion id=chatcmpl-81XRg12rOdAsk8PXxhxECedXxv5FF at 0x7da0eb3ea250> JSON: {\n","  \"id\": \"chatcmpl-81XRg12rOdAsk8PXxhxECedXxv5FF\",\n","  \"object\": \"chat.completion\",\n","  \"created\": 1695377584,\n","  \"model\": \"gpt-4-0613\",\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"As an AI, I don't have feelings, but I'm here and ready to assist you. How can I help you today?\"\n","      },\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"usage\": {\n","    \"prompt_tokens\": 13,\n","    \"completion_tokens\": 27,\n","    \"total_tokens\": 40\n","  },\n","  \"response_ms\": 3832.11\n","}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["response"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1695372879887,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"c5vrS_iBGEWl","outputId":"b1bab30c-18fc-41af-fa9b-a3a9fdf0ea17"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Hello! I am an AI language model, so I don't have feelings, but I'm here to help you. How can I assist you today?\""]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["response['choices'][0]['message']['content']"]},{"cell_type":"markdown","metadata":{"id":"FrLckcHRD4_Q"},"source":["# VertexAI / Google Palm\n","\n","LiteLLM also supports Google's PalM models through VertexAI\n","\n","**pre-requisite**\n","\n","Your Project ID litellm.vertex_project = \"hardy-device-38811\" Your Project ID\n","\n","Your Project Location litellm.vertex_location = \"us-central1\"\n","\n","**Learning on VertexAI**\n","\n","https://medium.com/generative-ai/google-palm-api-generative-models-for-code-generation-275589c5bd71"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5_xSDzGEgsr"},"outputs":[],"source":["!pip install google-cloud-aiplatform"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LN0Okg3dE-3s"},"outputs":[],"source":["# Authenticating user\n","from google.colab import auth as google_auth\n","google_auth.authenticate_user()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDYzKTvND4hq"},"outputs":[],"source":["import litellm\n","import os\n","from litellm import completion\n","litellm.vertex_project = \"generative-ai-training\" # Your Project ID\n","litellm.vertex_location = \"us-central1\"  # proj location\n","\n","messages=[{\"role\": \"user\", \"content\": \"write code for saying hi from LiteLLM\"}]\n","\n","response = completion(model=\"chat-bison\", messages = messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1695372887117,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"9Mqc3W0oD4aE","outputId":"1506c1ca-88d0-44a3-a0f1-b67d346ba50a"},"outputs":[{"data":{"text/plain":["<ModelResponse chat.completion id=chatcmpl-812932b3-2c83-4a6b-912b-d65aa6243141 at 0x7a09989ec4a0> JSON: {\n","  \"object\": \"chat.completion\",\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"message\": {\n","        \"content\": \" ```litellm\\ndef say_hi():\\n  print(\\\"Hi!\\\")\\n\\nsay_hi()\\n```\",\n","        \"role\": \"assistant\",\n","        \"logprobs\": null\n","      }\n","    }\n","  ],\n","  \"id\": \"chatcmpl-812932b3-2c83-4a6b-912b-d65aa6243141\",\n","  \"created\": 1695372887.6308095,\n","  \"response_ms\": 937.9159999999999,\n","  \"model\": \"chat-bison\",\n","  \"usage\": {\n","    \"prompt_tokens\": null,\n","    \"completion_tokens\": null,\n","    \"total_tokens\": null\n","  }\n","}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["response"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1695372894149,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"RwqlOsJTD4Rg","outputId":"bc06c745-3574-4dfa-ed24-39856dbfe97b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' ```litellm\\ndef say_hi():\\n  print(\"Hi!\")\\n\\nsay_hi()\\n```'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["response['choices'][0]['message']['content']"]},{"cell_type":"markdown","metadata":{"id":"MRhD-KISJydW"},"source":["# Anthropic\n","LiteLLM supports Claude-1, 1.2 and Claude-2."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3406,"status":"ok","timestamp":1695377631250,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"DNmF5n6gJ7iV","outputId":"84e66ccb-0c63-4e95-f4a1-27d512ec9f82"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"object\": \"chat.completion\",\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop_sequence\",\n","      \"index\": 0,\n","      \"message\": {\n","        \"content\": \" I'm doing well, thanks for asking! How about you?\",\n","        \"role\": \"assistant\",\n","        \"logprobs\": null\n","      }\n","    }\n","  ],\n","  \"id\": \"chatcmpl-f6885cab-33b7-4897-b4c4-583137e93d0e\",\n","  \"created\": 1695377630.360206,\n","  \"model\": \"claude-2\",\n","  \"usage\": {\n","    \"prompt_tokens\": 14,\n","    \"completion_tokens\": 13,\n","    \"total_tokens\": 27\n","  },\n","  \"response_ms\": 1799.288\n","}\n"]}],"source":["import os\n","from litellm import completion\n","\n","# set env - [OPTIONAL] replace with your anthropic key\n","os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-litellm-7_NPZhMGxY2GoHC59LgbDw\"\n","\n","messages = [{\"role\": \"user\", \"content\": \"Hey! how's it going?\"}]\n","response = completion(model=\"claude-2\", messages=messages)\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":300,"status":"ok","timestamp":1695373874279,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"gZBl2faCKEj6","outputId":"af56aa93-578a-4f9c-fa71-ba765a644732"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\" I'm doing well, thanks for asking!\""]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["response['choices'][0]['message']['content']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0ZJN5q4KQaK"},"outputs":[],"source":["# Streaming\n","messages = [{\"role\": \"user\", \"content\": \"Hey! how's it going?\"}]\n","st_response = completion(model=\"claude-instant-1\", messages=messages, stream=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1695374027121,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"gapTYQWKKog4","outputId":"592b7e8c-49bf-4847-93f1-2c91103cf5c4"},"outputs":[{"data":{"text/plain":["litellm.utils.CustomStreamWrapper"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["type(st_response)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1695374038775,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"r0wt7hunKrqD","outputId":"77528a2c-c185-43b6-ba3e-2ef7cb714457"},"outputs":[{"data":{"text/plain":["<litellm.utils.CustomStreamWrapper at 0x7a099897ce50>"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["st_response"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":409,"status":"ok","timestamp":1695374231639,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"PFJTsy5mKZ4e","outputId":"e9e75226-6dc1-43ca-f2c4-6c46bc9656e6"},"outputs":[{"name":"stdout","output_type":"stream","text":[" I\n","'m\n"," doing\n"," well\n",",\n"," thanks\n"," for\n"," asking\n","!\n"]}],"source":["for chunk in st_response:\n","    print(chunk[\"choices\"][0][\"delta\"][\"content\"])  # same as openai format"]},{"cell_type":"markdown","metadata":{"id":"YEO59nV2LCIg"},"source":["# AI21\n","LiteLLM supports j2-light, j2-mid and j2-ultra from AI21.\n","\n","They're available to use without a waitlist."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eWc4w5fLHtB"},"outputs":[],"source":["from litellm import completion\n","\n","# set env variable - [OPTIONAL] replace with your ai21 key\n","os.environ[\"AI21_API_KEY\"] = \"sk-litellm-7_NPZhMGxY2GoHC59LgbDw\"\n","\n","messages = [{\"role\": \"user\", \"content\": \"Write me a poem about the blue sky\"}]\n","\n","response = completion(model=\"j2-light\", messages=messages)"]},{"cell_type":"markdown","metadata":{"id":"oUeNgVa3L2TD"},"source":["#Together AI\n","LiteLLM supports all models on Together AI. Read Details on https://docs.litellm.ai/docs/providers/togetherai for all supported Models.\n","\n","You can modify the Model Name to experiment with different models hosted on TogetherAI Platform"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1774,"status":"ok","timestamp":1695374542232,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"cGbdUmh6LHp2","outputId":"2169104f-eb91-4435-db7f-02daea76440d"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"object\": \"chat.completion\",\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"length\",\n","      \"index\": 0,\n","      \"message\": {\n","        \"content\": \"\\n\\n\\nThe blue sky, a canvas of infinite possibility\\nAbove us\",\n","        \"role\": \"assistant\",\n","        \"logprobs\": null\n","      }\n","    }\n","  ],\n","  \"id\": \"chatcmpl-4f9619fb-0c78-45dc-9531-f584f7dec794\",\n","  \"created\": 1695374542.4376473,\n","  \"model\": \"togethercomputer/Llama-2-7B-32K-Instruct\",\n","  \"usage\": {\n","    \"prompt_tokens\": 14,\n","    \"completion_tokens\": 13,\n","    \"total_tokens\": 27\n","  },\n","  \"response_ms\": 1040.355\n","}\n"]}],"source":["from litellm import completion\n","\n","# set env variable - [OPTIONAL] replace with your together ai key\n","os.environ[\"TOGETHERAI_API_KEY\"] = \"sk-litellm-7_NPZhMGxY2GoHC59LgbDw\"\n","\n","messages = [{\"role\": \"user\", \"content\": \"Write me a poem about the blue sky\"}]\n","\n","response = completion(model=\"together_ai/togethercomputer/Llama-2-7B-32K-Instruct\", messages=messages)\n","\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":315,"status":"ok","timestamp":1695374544339,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"TxGO6o0aLHmv","outputId":"0c9b7bca-a92d-4d8d-e8d6-b9f773c84aaf"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n\\n\\nThe blue sky, a canvas of infinite possibility\\nAbove us'"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["response['choices'][0]['message']['content']"]},{"cell_type":"markdown","metadata":{"id":"neR7t6ATvSZ8"},"source":["# **Huggingface**\n","LiteLLM supports Huggingface models that use the text-generation-inference format or the Conversational task format.\n","\n","- Text-generation-interface: Here's all the models that use this format.\n","- Conversational task: Here's all the models that use this format.\n","- Non TGI/Conversational-task LLMs\n","\n","By default, we assume the you're trying to call models with the 'text-generation-interface' format (e.g. Llama2, Falcon, WizardCoder, MPT, etc.)\n","\n","This can be changed by setting task=\"conversational\" in the completion call. Example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNnoROxymPyH"},"outputs":[],"source":["import os\n","from litellm import completion\n","import litellm"]},{"cell_type":"markdown","metadata":{"id":"--gnG8MHHC23"},"source":["## Llama2 public Huggingface endpoint"]},{"cell_type":"markdown","metadata":{"id":"CIujStz9vpPH"},"source":["**Usage**\n","\n","You need to tell LiteLLM when you're calling Huggingface. Do that by setting it as part of the model name - completion(model=\"huggingface/<model_name>\",...).\n","\n","*We used the Falcon-7b and Llama2-7b LLM.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2Q_maLTnP_u"},"outputs":[],"source":["# [OPTIONAL] set env var\n","os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n","\n","model_falcon=\"huggingface/tiiuae/falcon-7b\"\n","model_llama = \"huggingface/NousResearch/llama-2-7b-chat-hf\""]},{"cell_type":"markdown","metadata":{"id":"0b5WG_INv003"},"source":["**Text-generation-interface (TGI) - Falcon-7b LLMs**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":859,"status":"ok","timestamp":1695297004100,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"sppWlzWVv7qL","outputId":"028770de-672b-421a-b02c-f86921888300"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"object\": \"chat.completion\",\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"message\": {\n","        \"content\": \"\\nLlamas are a very popular pet in the UK, but they are not native to the\",\n","        \"role\": \"assistant\",\n","        \"logprobs\": null\n","      }\n","    }\n","  ],\n","  \"id\": \"chatcmpl-3f19e7a7-a230-49bc-8f1c-c24927809200\",\n","  \"created\": 1695297005.7203326,\n","  \"response_ms\": 831.654,\n","  \"model\": \"tiiuae/falcon-7b\",\n","  \"usage\": {\n","    \"prompt_tokens\": 14,\n","    \"completion_tokens\": 20,\n","    \"total_tokens\": 34\n","  }\n","}\n"]}],"source":["messages = [{ \"content\": \"There's a llama in my garden 😱 What should I do?\",\"role\": \"user\"}]\n","\n","# e.g. Call 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints\n","response = completion(model=model_falcon, messages=messages)\n","\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1254,"status":"ok","timestamp":1695297008742,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"IBLf_hBUjUQu","outputId":"1b2057c8-7907-48be-d5c1-5599ffdc7276"},"outputs":[{"name":"stdout","output_type":"stream","text":["Content :  \n","Llamas are a very popular pet in the UK, but they are not native to the\n","Prompt Tokens 14 + Completion Tokens 20 = Total Tokens 34\n"]}],"source":["print(\"Content : \", response['choices'][0]['message']['content'])\n","print(f\"Prompt Tokens {response['usage']['prompt_tokens']} + Completion Tokens {response['usage']['completion_tokens']} = Total Tokens {response['usage']['total_tokens']}\")"]},{"cell_type":"markdown","metadata":{"id":"iTq0LpAlvuNv"},"source":["**Conversational-task (BlenderBot, etc.) - Blenderbot-400M LLMs**\n","\n","Key Change: completion(..., task=\"conversational\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12995,"status":"ok","timestamp":1695295228402,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"TFnmxZsg6xLP","outputId":"6592e60f-852e-44f8-c546-e103c0f4fbb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"object\": \"chat.completion\",\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"message\": {\n","        \"content\": \" I'm not sure what to do, but I do know that llamas are herbivorous mammals.\",\n","        \"role\": \"assistant\",\n","        \"logprobs\": null\n","      }\n","    }\n","  ],\n","  \"id\": \"chatcmpl-9f718bf5-2906-4028-920f-6ca10538269a\",\n","  \"created\": 1695295229.5946302,\n","  \"response_ms\": 12365.456,\n","  \"model\": \"facebook/blenderbot-400M-distill\",\n","  \"usage\": {\n","    \"prompt_tokens\": 14,\n","    \"completion_tokens\": 21,\n","    \"total_tokens\": 35\n","  }\n","}\n"]}],"source":["messages = [{ \"content\": \"There's a llama in my garden 😱 What should I do?\",\"role\": \"user\"}]\n","\n","# e.g. Call 'facebook/blenderbot-400M-distill' hosted on HF Inference endpoints\n","response = completion(model='huggingface/facebook/blenderbot-400M-distill', messages=messages, task=\"conversational\")\n","\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1695296506803,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"uf85c_jDeFdJ","outputId":"a77a19df-2a74-43f5-9688-02b343e34f33"},"outputs":[{"name":"stdout","output_type":"stream","text":["Content :   I'm not sure what to do, but I do know that llamas are herbivorous mammals.\n","Prompt Tokens 14 + Completion Tokens 21 = Total Tokens 35\n"]}],"source":["print(\"Content : \", response['choices'][0]['message']['content'])\n","print(f\"Prompt Tokens {response['usage']['prompt_tokens']} + Completion Tokens {response['usage']['completion_tokens']} = Total Tokens {response['usage']['total_tokens']}\")"]},{"cell_type":"markdown","metadata":{"id":"v4KDZcQ7zB3y"},"source":["**Models with Prompt Formatting - Blenderbot-400M LLM**\n","\n","For models with special prompt templates (e.g. Llama2), we format the prompt to fit their template.\n","\n","- What if we don't support a model you need? You can also specify you're own custom prompt formatting, in case we don't have your model covered yet.\n","\n","- Does this mean you have to specify a prompt for all models? No. By default we'll concatenate your message content to make a prompt.\n","\n","Custom prompt templates"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":974,"status":"ok","timestamp":1695296586299,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"RbAfXJy1jEqf","outputId":"c6103539-434c-4df4-b212-0ffb5c86af7e"},"outputs":[{"data":{"text/plain":["<ModelResponse chat.completion id=chatcmpl-becd7e26-dfe1-4b9b-93f0-1b02bc4f8fa8 at 0x7e9dcae20fe0> JSON: {\n","  \"object\": \"chat.completion\",\n","  \"choices\": [\n","    {\n","      \"finish_reason\": \"stop\",\n","      \"index\": 0,\n","      \"message\": {\n","        \"content\": \" I'm not sure what to do, but I do know that llamas are herbivorous mammals.\",\n","        \"role\": \"assistant\",\n","        \"logprobs\": null\n","      }\n","    }\n","  ],\n","  \"id\": \"chatcmpl-becd7e26-dfe1-4b9b-93f0-1b02bc4f8fa8\",\n","  \"created\": 1695296587.5900128,\n","  \"response_ms\": 772.165,\n","  \"model\": \"facebook/blenderbot-400M-distill\",\n","  \"usage\": {\n","    \"prompt_tokens\": 14,\n","    \"completion_tokens\": 21,\n","    \"total_tokens\": 35\n","  }\n","}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Create your own custom prompt template works\n","litellm.register_prompt_template(\n","        model = 'huggingface/facebook/blenderbot-400M-distill',\n","        roles={\n","            \"system\": {\n","                \"pre_message\": \"[INST] <<SYS>>\\n\",\n","                \"post_message\": \"\\n<</SYS>>\\n [/INST]\\n\"\n","            },\n","            \"user\": {\n","                \"pre_message\": \"[INST] \",\n","                \"post_message\": \" [/INST]\\n\"\n","            },\n","            \"assistant\": {\n","                \"post_message\": \"\\n\"\n","            }\n","        }\n","    )\n","\n","def test_huggingface_custom_model():\n","    response = completion(model='huggingface/facebook/blenderbot-400M-distill', messages=messages, task=\"conversational\")\n","    return response\n","\n","test_huggingface_custom_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1695296590772,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"QVV0I_f7jQXO","outputId":"16d24146-a624-4597-c4e0-973c6e8ccf31"},"outputs":[{"name":"stdout","output_type":"stream","text":["Content :   I'm not sure what to do, but I do know that llamas are herbivorous mammals.\n","Prompt Tokens 14 + Completion Tokens 21 = Total Tokens 35\n"]}],"source":["print(\"Content : \", response['choices'][0]['message']['content'])\n","print(f\"Prompt Tokens {response['usage']['prompt_tokens']} + Completion Tokens {response['usage']['completion_tokens']} = Total Tokens {response['usage']['total_tokens']}\")"]},{"cell_type":"markdown","metadata":{"id":"YNELRqMHhVXh"},"source":["# **Error Exploration**\n","\n","These types of issues are faced when using the Falcon-7b LLM."]},{"cell_type":"markdown","metadata":{"id":"gPxf964Zl-Qf"},"source":["**Text-generation-interface (TGI) - Llama2-7b LLMs**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":538},"executionInfo":{"elapsed":885,"status":"error","timestamp":1695297250915,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"NiJ5lfSHkUyL","outputId":"2413d4d7-4f96-403b-f15a-8c829b7931fb"},"outputs":[{"ename":"APIError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHuggingfaceError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, return_async, mock_response, api_key, api_version, api_base, force_timeout, num_beams, logger_fn, verbose, azure, custom_llm_provider, litellm_call_id, litellm_logging_obj, use_client, id, top_k, task, return_full_text, remove_input, request_timeout, fallbacks, caching, cache_params)\u001b[0m\n\u001b[1;32m    668\u001b[0m             )\n\u001b[0;32m--> 669\u001b[0;31m             model_response = huggingface_restapi.completion(\n\u001b[0m\u001b[1;32m    670\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/huggingface_restapi.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, api_base, model_response, print_verbose, encoding, api_key, logging_obj, custom_prompt_dict, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mprint_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"response.status_code: {response.status_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             raise HuggingfaceError(\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompletion_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHuggingfaceError\u001b[0m: The model NousResearch/Llama-2-7b-chat-hf is too large to be loaded automatically (13GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-ff8c18e52555>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# e.g. Call 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_llama\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[1;32m    650\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"stream\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/timeout.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mlocal_timeout_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"request_timeout\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_timeout_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/timeout.py\u001b[0m in \u001b[0;36masync_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masync_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LoopWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, return_async, mock_response, api_key, api_version, api_base, force_timeout, num_beams, logger_fn, verbose, azure, custom_llm_provider, litellm_call_id, litellm_logging_obj, use_client, id, top_k, task, return_full_text, remove_input, request_timeout, fallbacks, caching, cache_params)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   1072\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletion_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2438\u001b[0m         \u001b[0;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2440\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                         \u001b[0mexception_mapping_worked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m                         raise APIError(\n\u001b[0m\u001b[1;32m   2196\u001b[0m                             \u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m                             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"HuggingfaceException - {original_exception.message}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAPIError\u001b[0m: HuggingfaceException - The model NousResearch/Llama-2-7b-chat-hf is too large to be loaded automatically (13GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."]}],"source":["messages = [{ \"content\": \"There's a llama in my garden 😱 What should I do?\",\"role\": \"user\"}]\n","\n","# e.g. Call 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints\n","response = completion(model=model_llama, messages=messages)\n","\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":649},"executionInfo":{"elapsed":1032,"status":"error","timestamp":1695297402165,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"ThkS5B5rz81t","outputId":"ae2afd56-8462-440a-db15-12e844019277"},"outputs":[{"ename":"APIError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/huggingface_restapi.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, api_base, model_response, print_verbose, encoding, api_key, logging_obj, custom_prompt_dict, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mcompletion_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mHuggingfaceError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, return_async, mock_response, api_key, api_version, api_base, force_timeout, num_beams, logger_fn, verbose, azure, custom_llm_provider, litellm_call_id, litellm_logging_obj, use_client, id, top_k, task, return_full_text, remove_input, request_timeout, fallbacks, caching, cache_params)\u001b[0m\n\u001b[1;32m    668\u001b[0m             )\n\u001b[0;32m--> 669\u001b[0;31m             model_response = huggingface_restapi.completion(\n\u001b[0m\u001b[1;32m    670\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/huggingface_restapi.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, api_base, model_response, print_verbose, encoding, api_key, logging_obj, custom_prompt_dict, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             raise HuggingfaceError(\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHuggingfaceError\u001b[0m: Failed to deserialize the JSON body into the target type: inputs: invalid type: map, expected a string at line 1 column 11","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-98002421fc6e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"There's a llama in my garden 😱 What should I do?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_falcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conversational\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[1;32m    650\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"stream\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/timeout.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mlocal_timeout_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"request_timeout\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_timeout_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/timeout.py\u001b[0m in \u001b[0;36masync_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masync_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LoopWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, return_async, mock_response, api_key, api_version, api_base, force_timeout, num_beams, logger_fn, verbose, azure, custom_llm_provider, litellm_call_id, litellm_logging_obj, use_client, id, top_k, task, return_full_text, remove_input, request_timeout, fallbacks, caching, cache_params)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   1072\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletion_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2438\u001b[0m         \u001b[0;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2440\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                         \u001b[0mexception_mapping_worked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m                         raise APIError(\n\u001b[0m\u001b[1;32m   2196\u001b[0m                             \u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m                             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"HuggingfaceException - {original_exception.message}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAPIError\u001b[0m: HuggingfaceException - Failed to deserialize the JSON body into the target type: inputs: invalid type: map, expected a string at line 1 column 11"]}],"source":["messages = [{ \"content\": \"There's a llama in my garden 😱 What should I do?\",\"role\": \"user\"}]\n","\n","response = completion(model=model_falcon, messages=messages, task=\"conversational\")\n","\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":649},"executionInfo":{"elapsed":1909,"status":"error","timestamp":1695298003463,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"znVAE_nroXhz","outputId":"a948b0cc-8465-443e-d845-69f0e06c1e97"},"outputs":[{"ename":"APIError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/huggingface_restapi.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, api_base, model_response, print_verbose, encoding, api_key, logging_obj, custom_prompt_dict, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mcompletion_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mHuggingfaceError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, return_async, mock_response, api_key, api_version, api_base, force_timeout, num_beams, logger_fn, verbose, azure, custom_llm_provider, litellm_call_id, litellm_logging_obj, use_client, id, top_k, task, return_full_text, remove_input, request_timeout, fallbacks, caching, cache_params)\u001b[0m\n\u001b[1;32m    668\u001b[0m             )\n\u001b[0;32m--> 669\u001b[0;31m             model_response = huggingface_restapi.completion(\n\u001b[0m\u001b[1;32m    670\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/llms/huggingface_restapi.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, api_base, model_response, print_verbose, encoding, api_key, logging_obj, custom_prompt_dict, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             raise HuggingfaceError(\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHuggingfaceError\u001b[0m: Failed to deserialize the JSON body into the target type: inputs: invalid type: map, expected a string at line 1 column 11","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-93eb27c0c069>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtest_huggingface_custom_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-39-93eb27c0c069>\u001b[0m in \u001b[0;36mtest_huggingface_custom_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_huggingface_custom_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_falcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conversational\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[1;32m    650\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"stream\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/timeout.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mlocal_timeout_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"request_timeout\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_timeout_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/timeout.py\u001b[0m in \u001b[0;36masync_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masync_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LoopWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, return_async, mock_response, api_key, api_version, api_base, force_timeout, num_beams, logger_fn, verbose, azure, custom_llm_provider, litellm_call_id, litellm_logging_obj, use_client, id, top_k, task, return_full_text, remove_input, request_timeout, fallbacks, caching, cache_params)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   1072\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletion_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2438\u001b[0m         \u001b[0;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2440\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                         \u001b[0mexception_mapping_worked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m                         raise APIError(\n\u001b[0m\u001b[1;32m   2196\u001b[0m                             \u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m                             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"HuggingfaceException - {original_exception.message}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAPIError\u001b[0m: HuggingfaceException - Failed to deserialize the JSON body into the target type: inputs: invalid type: map, expected a string at line 1 column 11"]}],"source":["# Create your own custom prompt template works\n","litellm.register_prompt_template(\n","        model_falcon,\n","        roles={\n","            \"system\": {\n","                \"pre_message\": \"[INST] <<SYS>>\\n\",\n","                \"post_message\": \"\\n<</SYS>>\\n [/INST]\\n\"\n","            },\n","            \"user\": {\n","                \"pre_message\": \"[INST] \",\n","                \"post_message\": \" [/INST]\\n\"\n","            },\n","            \"assistant\": {\n","                \"post_message\": \"\\n\"\n","            }\n","        }\n","    )\n","\n","def test_huggingface_custom_model():\n","    response = completion(model=model_falcon, messages=messages, task=\"conversational\")\n","    print(response['choices'][0]['message']['content'])\n","    return response\n","\n","test_huggingface_custom_model()"]},{"cell_type":"markdown","metadata":{"id":"0-Vq-EqeZkqH"},"source":["# OpenRouter\n","\n","LiteLLM also supports models hosted by OpenRouter.\n","\n","https://docs.litellm.ai/docs/providers/openrouter"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5954,"status":"ok","timestamp":1695948686752,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"EsQiIJBrhikR"},"outputs":[],"source":["import os\n","from litellm import completion\n","os.environ[\"OPENROUTER_API_KEY\"] = \"\"\n","\n","messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n","\n","response = completion(\n","            model=\"openrouter/openai/gpt-4\",\n","            messages=messages,\n","        )"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1695948686753,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"7M0rpoj3alZC","outputId":"00228249-029d-48bf-8411-db5a366f2e9c"},"outputs":[{"data":{"text/plain":["<OpenAIObject id=gen-je3Qttz9haQRN30DUb1GD1F2cbsO at 0x7d1ee82c13a0> JSON: {\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"As an artificial intelligence, I don't have feelings, but I'm functioning as expected. Thanks for asking! How can I assist you today?\"\n","      },\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"model\": \"gpt-4-0613\",\n","  \"usage\": {\n","    \"prompt_tokens\": 13,\n","    \"completion_tokens\": 29,\n","    \"total_tokens\": 42\n","  },\n","  \"id\": \"gen-je3Qttz9haQRN30DUb1GD1F2cbsO\"\n","}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["response"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6016,"status":"ok","timestamp":1695948741506,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"Zedj3RJaanoi","outputId":"eb3859c6-debf-4d68-9173-3ac4d1d43821"},"outputs":[{"data":{"text/plain":["<OpenAIObject id=gen-pZ3jQLYmSPCZF73hvxNnTW3bHJYP at 0x7d1f01a39710> JSON: {\n","  \"id\": \"gen-pZ3jQLYmSPCZF73hvxNnTW3bHJYP\",\n","  \"model\": \"chat-bison@001\",\n","  \"choices\": [\n","    {\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"I am doing well, thank you for asking.\"\n","      }\n","    }\n","  ]\n","}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Google Palm-2-chat-bision\n","response = completion(\n","            model=\"openrouter/google/palm-2-chat-bison\",\n","            messages=messages,\n","        )\n","response"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcYuESi2a_7K"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
