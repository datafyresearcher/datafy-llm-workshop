{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a target=\"_blank\" href=\"https://colab.research.google.com/github/amjadraza/datafy-llm-workshop/blob/main/notebooks/01_prompt_engineering.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"moZWfnmqsRJC"},"source":["# **Best Practices around Prompt Engineering**\n","\n","1. **Define Clear Objectives:** Clearly articulate the goals and objectives you want to achieve with your prompt, such as generating specific types of content or eliciting certain responses.\n","\n","2. **Understand the Model:** Familiarize yourself with the language model you'll be using, its capabilities, and limitations. This includes knowing the model's training data and potential biases.\n","\n","3. **Craft Specific Prompts:** Design prompts that are tailored to your objectives. Use clear and concise language, and consider including context or constraints to guide the model's output.\n","\n","4. **Experiment and Iterate:** It's often necessary to experiment with different prompts and iterations to achieve the desired results. Be prepared to refine and adjust your prompts based on the model's responses.\n","\n","5. **Ethical Considerations:** Pay attention to ethical considerations, such as avoiding biased or harmful prompts, and ensure that the content generated aligns with ethical guidelines.\n","\n","6. **Evaluation and Testing:** Establish methods for evaluating the quality of model outputs. This may involve human review, automated metrics, or other evaluation techniques.\n","\n","7. **Fine-Tuning:** If necessary, consider fine-tuning the model on a specific dataset to improve its performance on your prompts.\n","\n","8. **Documentation:** Keep thorough documentation of your prompts and the model's responses for transparency and reproducibility.\n","\n","9. **Monitor and Maintain:** Continuously monitor the model's performance and adapt your prompts as needed over time.\n","\n","10. **Collaborate and Share:** Share insights and best practices with the community to contribute to the collective knowledge of prompt engineering.\n","\n","- [Open AI: Best practices for prompt engineering with OpenAI API](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n","- [Google: Tips to enhance your prompt-engineering abilities](https://cloud.google.com/blog/products/application-development/five-best-practices-for-prompt-engineering)\n","\n","These steps can serve as a framework for effective prompt engineering, but the specific approach may vary depending on the context and goals of your project."]},{"cell_type":"markdown","metadata":{"id":"nn6L3F8hv5MI"},"source":["# **Schema**\n","\n","- Description: Covers basic data types and schemas used in the codebase.\n","\n","**ChatMessages:**\n","\n","- Description: Describes the primary interface for user interaction with chat messages, particularly relevant for AI systems.\n","\n","- Message Types:\n","  - SystemChatMessage: Contains instructions for the AI system.\n","  - HumanChatMessage: Represents input from a human interacting with the AI system.\n","  - AIChatMessage: Represents information generated by the AI system.\n","- Supported Users: System, Human, and AI.\n","\n","**Text:**\n","\n","- Description: Emphasizes that text is the primary means of interaction with language models.\n","- Note: Many interfaces in LangChain are centered around text.\n","\n","**Examples:**\n","- Description: Explains the concept of input/output pairs used for training and evaluating models.\n","- Types:\n","  - Model Examples: Used for fine-tuning a model.\n","  - Chain Examples: Used for evaluating end-to-end chains or training a model to replace a chain.\n","\n","**Document:**\n","- Description: Defines a piece of unstructured data consisting of page_content (data content) and metadata (additional information about the data).\n","\n","### **For Schema Code Example: Follow the bock of Chat Prompt Template.**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19883,"status":"ok","timestamp":1695949084911,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"},"user_tz":-600},"id":"LTLt2_WscIIp","outputId":"d2fa8995-ab77-4ead-a588-9f6234e0f611"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on CoLab\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["if 'google.colab' in str(get_ipython()):\n","  print('Running on CoLab')\n","  # Installation the LangChain Package\n","  !pip install -qU langchain transformers\n","else:\n","  print('Not running on CoLab')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElXPtDPP4vCf"},"outputs":[],"source":["import langchain\n","from transformers import AutoModelWithLMHead, AutoTokenizer\n","from langchain.llms import HuggingFaceHub\n","from langchain.chains import LLMChain\n","\n","from langchain.prompts import PromptTemplate\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.prompts import FewShotChatMessagePromptTemplate\n","from langchain.prompts.few_shot import FewShotPromptTemplate\n","from langchain.prompts.prompt import PromptTemplate\n","from langchain.prompts import load_prompt\n","from langchain.prompts.pipeline import PipelinePromptTemplate\n","from langchain.output_parsers import CommaSeparatedListOutputParser\n","\n","import os, json, warnings\n","from datetime import datetime\n","# Settings the warnings to be ignored\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUx8BT5_5jD0"},"outputs":[],"source":["os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'"]},{"cell_type":"markdown","metadata":{"id":"fHHAbToCD992"},"source":["#### **LLMChain**\n","- Chain to run queries against LLMs.\n","\n","#####  **Flan, by Google**\n","\n","Below is the example of LLM model you can access through the Hugging Face Hub integration."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yxta9aEJDk0Q"},"outputs":[],"source":["def LLMChain_Prompt(Prompt):\n","  repo_id = \"google/flan-t5-xxl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n","\n","  llm = HuggingFaceHub(\n","      repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 128}\n","  )\n","  llm_chain = LLMChain(prompt=Prompt, llm=llm)\n","  return llm_chain.run(Prompt.template)\n","\n","\n","def LLMChain_PromptChat(Chat,Prompt):\n","  repo_id = \"google/flan-t5-xxl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n","\n","  llm = HuggingFaceHub(\n","      repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n","  )\n","  llm_chain = LLMChain(prompt=Prompt, llm=llm)\n","  return llm_chain.run(Chat)"]},{"cell_type":"markdown","metadata":{"id":"CrV-Uv-yb56a"},"source":["#### **Template Formats**\n","\n","PromptTemplate by default uses Python f-string as its template format. However, it can also use other formats like jinja2, specified through the template_format argument."]},{"cell_type":"markdown","metadata":{"id":"e1H4c1Azd5lt"},"source":["**To use the jinja2 template:**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":873,"status":"ok","timestamp":1695199810179,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"ql3dsqS0cHMr","outputId":"00acbee2-886a-4d9d-faa7-606d8643dcdb"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Who won the FIFA World Cup in the year 1994?'"]},"execution_count":210,"metadata":{},"output_type":"execute_result"}],"source":["jinja2_template = \"Who won the {{Cup}} World Cup in the year 1994?\"\n","prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\")\n","\n","message = prompt.format(Cup=\"FIFA\")\n","message"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":970,"status":"ok","timestamp":1695199811700,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"Rsr9CQ0Lcme8","outputId":"cf4aeac7-89b1-4c61-b7fb-2cd866f91685"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'brazil'"]},"execution_count":211,"metadata":{},"output_type":"execute_result"}],"source":["LLMChain_Prompt(Prompt = prompt)"]},{"cell_type":"markdown","metadata":{"id":"tJb2Tq5OeAIg"},"source":["**To use the Python f-string template:**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1695199812396,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"kLWfFqbAeBen","outputId":"6dbddb3d-92cf-4d44-93eb-3725bfd19a41"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Who won the FIFA World Cup in the year 1994?'"]},"execution_count":212,"metadata":{},"output_type":"execute_result"}],"source":["fstring_template = \"\"\"Who won the {Cup} World Cup in the year 1994?\"\"\"\n","prompt = PromptTemplate.from_template(fstring_template)\n","\n","message = prompt.format(Cup=\"FIFA\")\n","message"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":765,"status":"ok","timestamp":1695199813702,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"05nlXnq4eJt3","outputId":"7c4f63ae-2bba-41ad-93d5-3bfde835be94"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'France'"]},"execution_count":213,"metadata":{},"output_type":"execute_result"}],"source":["LLMChain_Prompt(Prompt = prompt)"]},{"cell_type":"markdown","metadata":{"id":"33WTviajgG67"},"source":["# **Output Parsers**\n","\n","#### **List parser**\n","This output parser can be used when you want to return a list of comma-separated items."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1431,"status":"ok","timestamp":1695199835863,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"1tbl55xlbt4q","outputId":"2ebbebfc-4173-4ae0-9ee0-f5e5f0ceefe4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output Parser LIST :  ['foo', 'bar', 'baz']\n"]}],"source":["output_parser = CommaSeparatedListOutputParser()\n","\n","prompt = PromptTemplate(\n","    template=\"List five {subject}.\\n{format_instructions}\",\n","    input_variables=[\"subject\"],\n","    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",")\n","\n","_input = prompt.format(subject=\"ice cream flavors\")\n","\n","output = LLMChain_Prompt(Prompt = prompt)\n","\n","print(\"Output Parser LIST : \", output_parser.parse(output))"]},{"cell_type":"markdown","metadata":{"id":"0SSVRbUGCzAt"},"source":["# **PromptTemplate**\n","- A prompt template for a language model.\n","- A prompt template consists of a string template. It accepts a set of parameters from the user that can be used to generate a prompt for a language model.\n","- The template can be formatted using either f-strings (default) or jinja2 syntax.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqgZWFkMFcHB"},"outputs":[],"source":["question = \"Who won the FIFA World Cup in the year 1994?\""]},{"cell_type":"markdown","metadata":{"id":"V7DCXsxsGH8G"},"source":["**1-Method**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRjZ-EqxDSUf"},"outputs":[],"source":["# For additional validation, specify input_variables explicitly.\n","# These variables will be compared against the variables present in the template string during instantiation, raising an exception if there is a mismatch.\n","\n","template = \"\"\"\n","Question: {question}\n","Answer: Let's think step by step.\n","\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":2403,"status":"ok","timestamp":1695199922527,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"Mehb-9DXH6Fc","outputId":"db2f97c6-feaf-4f4a-c4a6-7f5784967132"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The 1994 FIFA World Cup was held in the United States. The United States did not qualify for the'"]},"execution_count":221,"metadata":{},"output_type":"execute_result"}],"source":["LLMChain_PromptChat(Chat = question, Prompt = prompt)"]},{"cell_type":"markdown","metadata":{"id":"UH_Jh_PyGMJj"},"source":["**2-Method**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":856,"status":"ok","timestamp":1695200079284,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"1eUr__chFEXs","outputId":"a07ce10d-303d-4e93-9d59-2edf28783d6d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nQuestion: Who won the FIFA World Cup in the year 1994?. \\n\\nAnswer: Let's think step by step.\""]},"execution_count":223,"metadata":{},"output_type":"execute_result"}],"source":["# By default, PromptTemplate uses Python's str.format syntax for templating; however other templating syntax is available (e.g., jinja2).\n","\n","prompt1 = PromptTemplate.from_template(\"\"\"\n","Question: {question}.\n","\n","Answer: Let's think step by step.\"\"\")\n","prompt1.format(question=question)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":2099,"status":"ok","timestamp":1695200081900,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"tdAtREyIEWng","outputId":"334fdb79-09d0-4131-c41e-595bca589c1d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The FIFA World Cup was first played in 1930. The 1994 FIFA World Cup was the 20th FIFA'"]},"execution_count":224,"metadata":{},"output_type":"execute_result"}],"source":["LLMChain_PromptChat(Chat = question, Prompt = prompt1)"]},{"cell_type":"markdown","metadata":{"id":"GLZfyU-UNt7A"},"source":["# **Chat Prompt Template**\n","\n","The prompt to chat models is a list of chat messages.\n","\n","Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role.\n","\n","*Create a chat prompt template like this:*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1695200112973,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"fC3qqQQeIRrH","outputId":"5cec5399-411a-4966-c52b-516ddcb53c90"},"outputs":[{"data":{"text/plain":["[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}),\n"," HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, example=False),\n"," AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, example=False),\n"," HumanMessage(content='What is your name?', additional_kwargs={}, example=False)]"]},"execution_count":225,"metadata":{},"output_type":"execute_result"}],"source":["prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a helpful AI bot. Your name is Bob.\"),\n","    (\"human\", \"Hello, how are you doing?\"),\n","    (\"ai\", \"I'm doing well, thanks!\"),\n","    (\"human\", \"{user_input}\"),\n","])\n","messages = prompt.format_messages(\n","    user_input=\"What is your name?\"\n",")\n","messages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":2105,"status":"ok","timestamp":1695200124237,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"9P_yqszNMsEC","outputId":"976d90e6-e4b4-41bf-a004-9df538b9b92f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Hello, how are you doing? What is your name?'"]},"execution_count":226,"metadata":{},"output_type":"execute_result"}],"source":["LLMChain_PromptChat(Chat = messages, Prompt = prompt)"]},{"cell_type":"markdown","metadata":{"id":"lgKmmPJMSz94"},"source":["# **Few Shot Prompt Template**\n","Create the example set:\n"," - To get started, create a list of few-shot examples. Each example should be a dictionary with the keys being the input variables and the values being the values for those input variables."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDY3T8olM1Q3"},"outputs":[],"source":["examples = [\n","  {\n","    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n","    \"answer\":\n","\"\"\"\n","Are follow up questions needed here: Yes.\n","Follow up: How old was Muhammad Ali when he died?\n","Intermediate answer: Muhammad Ali was 74 years old when he died.\n","Follow up: How old was Alan Turing when he died?\n","Intermediate answer: Alan Turing was 41 years old when he died.\n","So the final answer is: Muhammad Ali\n","\"\"\"\n","  },\n","  {\n","    \"question\": \"When was the founder of craigslist born?\",\n","    \"answer\":\n","\"\"\"\n","Are follow up questions needed here: Yes.\n","Follow up: Who was the founder of craigslist?\n","Intermediate answer: Craigslist was founded by Craig Newmark.\n","Follow up: When was Craig Newmark born?\n","Intermediate answer: Craig Newmark was born on December 6, 1952.\n","So the final answer is: December 6, 1952\n","\"\"\"\n","  },\n","  {\n","    \"question\": \"Who was the maternal grandfather of George Washington?\",\n","    \"answer\":\n","\"\"\"\n","Are follow up questions needed here: Yes.\n","Follow up: Who was the mother of George Washington?\n","Intermediate answer: The mother of George Washington was Mary Ball Washington.\n","Follow up: Who was the father of Mary Ball Washington?\n","Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n","So the final answer is: Joseph Ball\n","\"\"\"\n","  },\n","  {\n","    \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n","    \"answer\":\n","\"\"\"\n","Are follow up questions needed here: Yes.\n","Follow up: Who is the director of Jaws?\n","Intermediate Answer: The director of Jaws is Steven Spielberg.\n","Follow up: Where is Steven Spielberg from?\n","Intermediate Answer: The United States.\n","Follow up: Who is the director of Casino Royale?\n","Intermediate Answer: The director of Casino Royale is Martin Campbell.\n","Follow up: Where is Martin Campbell from?\n","Intermediate Answer: New Zealand.\n","So the final answer is: No\n","\"\"\"\n","  }\n","]"]},{"cell_type":"markdown","metadata":{"id":"CXmj4zKQTGy0"},"source":["Create a formatter for the few-shot examples:\n"," - Configure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1695200262094,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"brYoyVUSQtce","outputId":"1e7d26cc-abd6-4884-aa9d-6912c38c0856"},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: Who lived longer, Muhammad Ali or Alan Turing?\n","\n","Are follow up questions needed here: Yes.\n","Follow up: How old was Muhammad Ali when he died?\n","Intermediate answer: Muhammad Ali was 74 years old when he died.\n","Follow up: How old was Alan Turing when he died?\n","Intermediate answer: Alan Turing was 41 years old when he died.\n","So the final answer is: Muhammad Ali\n","\n"]}],"source":["example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")\n","\n","print(example_prompt.format(**examples[0]))"]},{"cell_type":"markdown","metadata":{"id":"vi2HNZ_1TOxx"},"source":["Feed examples and formatter to FewShotPromptTemplate:\n"," - Finally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZH553Y0Qwn-"},"outputs":[],"source":["question_fewshot_prompt = \"Who was the father of Mary Ball Washington?\"\n","\n","prompt = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=example_prompt,\n","    suffix=\"Question: {input}\",\n","    input_variables=[\"input\"]\n",")\n","\n","messages = prompt.format(input=question_fewshot_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":999,"status":"ok","timestamp":1695200271391,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"lMZ8CfJuQ5q7","outputId":"aa70e064-f0ea-4053-a60d-1c90fd254c5a"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Intermediate answer: Joseph Ball'"]},"execution_count":234,"metadata":{},"output_type":"execute_result"}],"source":["LLMChain_PromptChat(Chat = question_fewshot_prompt, Prompt = prompt)"]},{"cell_type":"markdown","metadata":{"id":"jvKAVeLwaWh3"},"source":["# **Few Shot for Chat Model**\n","\n","The goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.\n","\n","The most basic (and common) few-shot prompting technique is to use a fixed prompt example. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production.\n","\n","The basic components of the template are:\n","\n"," - examples: A list of dictionary examples to include in the final prompt.\n"," - example_prompt: converts each example into 1 or more messages through its format_messages method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-DisOH9ZFu6"},"outputs":[],"source":["# Then, define the examples you'd like to include.\n","examples = [\n","    {\"input\": \"2+2\", \"output\": \"4\"},\n","    {\"input\": \"2+3\", \"output\": \"5\"},\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9JIOD7oSRTOS"},"outputs":[],"source":["# Next, assemble them into the few-shot prompt template.\n","# This is a prompt template used to format each individual example.\n","example_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"human\", \"{input}\"),\n","        (\"ai\", \"{output}\"),\n","    ]\n",")\n","few_shot_prompt = FewShotChatMessagePromptTemplate(\n","    example_prompt=example_prompt,\n","    examples=examples,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGJT9-lTZMBk"},"outputs":[],"source":["# Finally, assemble your final prompt with format message and use it with a model.\n","\n","final_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"You are a wondrous wizard of math.\"),\n","        few_shot_prompt,\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","\n","messages = final_prompt.format_messages(\n","    input=\"What is the wordrous wizard of math?\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":1194,"status":"ok","timestamp":1695200394732,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"RzIcbpmvZOxk","outputId":"550b24be-7457-43a3-fd7b-ebad88ed169d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'AI: 4 and 5'"]},"execution_count":239,"metadata":{},"output_type":"execute_result"}],"source":["question_fewshot_chat_prompt = \"What's the 2+2 and 2+3?\"\n","LLMChain_PromptChat(Chat = question_fewshot_chat_prompt, Prompt = final_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ku503xI8Z8UU"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"LQRmWlF2iVWq"},"source":["# **Partial Prompt Template**\n","\n","The use case for this is when you have a variable you know that you always want to fetch in a common way.\n","\n"," - A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables is a bit annoying.\n","\n","In this case, it's very handy to be able to partial the prompt with a function that always returns the current date."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pb3qpfwEg39U"},"outputs":[],"source":["def _get_datetime():\n","    now = datetime.now()\n","    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":825,"status":"ok","timestamp":1695200614720,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"-3JdDxGVg356","outputId":"7311c9b7-0ff2-4e91-e3b7-439be9444b3f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tell me now about the 09/20/2023, 09:03:35'"]},"execution_count":243,"metadata":{},"output_type":"execute_result"}],"source":["prompt = PromptTemplate(\n","    template=\"Tell me {current} about the {daytime}\",\n","    input_variables=[\"current\"],\n","    partial_variables={\"daytime\": _get_datetime}\n",");\n","message = prompt.format(current=\"now\")\n","message"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":186455,"status":"ok","timestamp":1695200803351,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"7ZTFtUehhLJz","outputId":"4ace11ac-c4bf-42c1-d92e-e8916cb4c517"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'About the 09/20/2023, 09:03:38'"]},"execution_count":244,"metadata":{},"output_type":"execute_result"}],"source":["Question = \"What is now the day time?\"\n","LLMChain_PromptChat(Chat = Question, Prompt = prompt)"]},{"cell_type":"markdown","metadata":{"id":"FoQvE6KdoWem"},"source":["# **Serialization**\n","\n","It is often preferrable to store prompts not as python code but as files. This can make it easy to share, store, and version prompts.\n","\n","At a high level, the following design principles are applied to serialization:\n","\n"," - Both JSON and YAML are supported. We want to support serialization methods that are human readable on disk, and YAML and JSON are two of the most popular methods for that. Note that this rule applies to prompts. For other assets, like examples, different serialization methods may be supported."]},{"cell_type":"markdown","metadata":{"id":"2ikfnq4-mMGI"},"source":["**PromptTemplate**\n","\n","This section covers examples for loading a PromptTemplate.\n","\n","**Loading from JSON**\n","\n","This shows an example of loading a PromptTemplate from JSON."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WCK2EssumKh_"},"outputs":[],"source":["data = {\n","    \"_type\": \"prompt\",\n","    \"input_variables\": [\"Cup\"],\n","    \"template\": \"Who won the {Cup} World Cup in the year 1994?.\"\n","}\n","\n","# Step 2: Convert the dictionary to a JSON-formatted string\n","json_data = json.dumps(data, indent=4)  # The 'indent' parameter is optional for pretty formatting\n","\n","# Step 3: Write the JSON string to a file\n","with open(\"simple_prompt.json\", \"w\") as json_file:\n","  json_file.write(json_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1695200866843,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"S-cLogbEm7qg","outputId":"7ea0b7fa-0a11-47dc-86b7-ec29cef2ca8a"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Who won the FIFA World Cup in the year 1994?.'"]},"execution_count":246,"metadata":{},"output_type":"execute_result"}],"source":["prompt = load_prompt(\"simple_prompt.json\")\n","message = prompt.format(Cup=\"FIFA\")\n","message"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ag_ZagTenaEc"},"outputs":[],"source":["josn_template = \"\"\"Who won the {Cup} World Cup in the year 1994?\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":1516,"status":"ok","timestamp":1695200878043,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"-db0ggU9naAg","outputId":"c5bedf8b-24ae-4acf-cd58-b5523ef31d0b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'France'"]},"execution_count":249,"metadata":{},"output_type":"execute_result"}],"source":["LLMChain_PromptChat(Chat = josn_template, Prompt = prompt)"]},{"cell_type":"markdown","metadata":{"id":"Qw3CvhA0lk-c"},"source":["# **Composition**\n","Compose multiple prompts together. This can be useful when you want to reuse parts of prompts. This can be done with a PipelinePrompt.\n","\n","A PipelinePrompt consists of two main parts:\n","\n"," - Final prompt: The final prompt that is returned\n"," - Pipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeuHN9afjuir"},"outputs":[],"source":["full_template = \"\"\"\n","{introduction}\n","\n","{example}\n","\n","{start}\n","\"\"\"\n","full_prompt = PromptTemplate.from_template(full_template)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmpHc5pPj0v6"},"outputs":[],"source":["introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n","introduction_prompt = PromptTemplate.from_template(introduction_template)\n","\n","example_template = \"\"\"Here's an example of an interaction:\n","\n","Q: {example_q}\n","A: {example_a}\"\"\"\n","example_prompt = PromptTemplate.from_template(example_template)\n","\n","\n","start_template = \"\"\"Now, do this for real!\n","\n","Q: {input}\n","A:\"\"\"\n","start_prompt = PromptTemplate.from_template(start_template)\n","\n","\n","input_prompts = [\n","    (\"introduction\", introduction_prompt),\n","    (\"example\", example_prompt),\n","    (\"start\", start_prompt)\n","]\n","pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1695200898317,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"Bi0dBu6Oj5lM","outputId":"debe2991-e92d-4bf6-d563-1dc60ef6739d"},"outputs":[{"data":{"text/plain":["['person', 'input', 'example_a', 'example_q']"]},"execution_count":252,"metadata":{},"output_type":"execute_result"}],"source":["pipeline_prompt.input_variables"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1695200899917,"user":{"displayName":"Hassan Raza","userId":"12265837829679079048"},"user_tz":-300},"id":"XqpBnBm8kSf0","outputId":"286618fe-f47c-4f79-c6b9-85f9defb50b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","You are impersonating Elon Musk.\n","\n","Here's an example of an interaction:\n","\n","Q: What's your favorite car?\n","A: Tesla\n","\n","Now, do this for real!\n","\n","Q: What's your favorite social media site?\n","A:\n","\n"]}],"source":["print(pipeline_prompt.format(\n","    person=\"Elon Musk\",\n","    example_q=\"What's your favorite car?\",\n","    example_a=\"Tesla\",\n","    input=\"What's your favorite social media site?\"\n","))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMDMOPRWlN2v"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
