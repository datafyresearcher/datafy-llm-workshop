{"cells":[{"cell_type":"markdown","id":"a786c77c","metadata":{"id":"a786c77c"},"source":["# LangChain: Memory\n","\n","Memory in LLM refers to the ability of the language model to remember and store information about specific entities in a conversation. It uses an LLM (Large Language Model) to extract information on entities and builds up its knowledge about those entities over time. The memory component includes short-term memory for in-context learning and long-term memory for retaining and recalling information over extended periods. It can be used to store facts and details about entities mentioned in the conversation, allowing the language model to provide more accurate and relevant responses. The ConversationEntityMemory class in the Langchain library provides functionality for implementing entity memory in a conversation.\n","\n","## Outline\n","* ConversationBufferMemory\n","* ConversationBufferWindowMemory\n","* ConversationTokenBufferMemory\n","* ConversationSummaryMemory\n","\n","https://python.langchain.com/docs/integrations/memory\n"]},{"cell_type":"code","source":["if 'google.colab' in str(get_ipython()):\n","  print('Running on CoLab')\n","  # Install the package\n","  !pip install langchain openai litellm tiktoken -qU\n","else:\n","  print('Not running on CoLab')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5IyRc7RRpPC","executionInfo":{"status":"ok","timestamp":1695946306750,"user_tz":-600,"elapsed":15021,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"e1fbe004-226f-4daf-f551-fe769d94342a"},"id":"j5IyRc7RRpPC","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["#OpenAI API\n","\n","Let us try first using Openai API directly\n","\n","Read about the OpenAI API specifications and endpoints on below link\n","\n","https://platform.openai.com/docs/api-reference\n","\n","To learn about the available Models\n","\n","https://platform.openai.com/docs/models/gpt-3-5"],"metadata":{"id":"jrukJEw2RtLt"},"id":"jrukJEw2RtLt"},{"cell_type":"code","source":["import os\n","import openai\n","\n","# select the model to use\n","llm_model = \"gpt-3.5-turbo-0613\"\n","\n","# openai.api_key = \"\"\n","\n","os.environ['OPENAI_API_KEY'] = \"\"\n","\n","# os.environ[\"OPENAI_API_KEY\"] = \"sk-litellm-7_NPZhMGxY2GoHC59LgbDw\" # [OPTIONAL] replace with your openai key"],"metadata":{"id":"1nILzxxVRxxv"},"id":"1nILzxxVRxxv","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1297dcd5","metadata":{"id":"1297dcd5"},"source":["## ConversationBufferMemory"]},{"cell_type":"code","execution_count":null,"id":"20ad6fe2","metadata":{"tags":[],"id":"20ad6fe2"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory\n"]},{"cell_type":"code","execution_count":null,"id":"88bdf13d","metadata":{"tags":[],"id":"88bdf13d"},"outputs":[],"source":["llm = ChatOpenAI(temperature=0.0, model=llm_model)\n","memory = ConversationBufferMemory()\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"db24677d","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":268},"id":"db24677d","executionInfo":{"status":"ok","timestamp":1695946365634,"user_tz":-600,"elapsed":1872,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"adb08cee-c02d-4217-9aad-118cbb44c645"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hi, my name is Andrew\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Hello Andrew! It's nice to meet you. How can I assist you today?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["conversation.predict(input=\"Hi, my name is Andrew\")"]},{"cell_type":"code","execution_count":null,"id":"cc3ef937","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"cc3ef937","executionInfo":{"status":"ok","timestamp":1695946372260,"user_tz":-600,"elapsed":1093,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"cb4b19f7-8d94-403d-a957-21dad3e5cbba"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Andrew\n","AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n","Human: What is 1+1?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'1+1 is equal to 2.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["conversation.predict(input=\"What is 1+1?\")"]},{"cell_type":"code","execution_count":null,"id":"acf3339a","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"acf3339a","executionInfo":{"status":"ok","timestamp":1695946376248,"user_tz":-600,"elapsed":830,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"a2d522de-37eb-4fcc-c653-5b8d2301689a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Andrew\n","AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n","Human: What is 1+1?\n","AI: 1+1 is equal to 2.\n","Human: What is my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Your name is Andrew.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["conversation.predict(input=\"What is my name?\")"]},{"cell_type":"code","execution_count":null,"id":"2529400d","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"2529400d","executionInfo":{"status":"ok","timestamp":1695946393808,"user_tz":-600,"elapsed":326,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"95f8d909-9d7b-48a0-cb38-312167b54506"},"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Hi, my name is Andrew\n","AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n","Human: What is 1+1?\n","AI: 1+1 is equal to 2.\n","Human: What is my name?\n","AI: Your name is Andrew.\n"]}],"source":["print(memory.buffer)"]},{"cell_type":"code","execution_count":null,"id":"5018cb0a","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"5018cb0a","executionInfo":{"status":"ok","timestamp":1695946398083,"user_tz":-600,"elapsed":330,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"f4d412d5-83a8-4b80-d6c6-474ac72f2ec4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Andrew.\"}"]},"metadata":{},"execution_count":9}],"source":["memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":null,"id":"14219b70","metadata":{"tags":[],"id":"14219b70"},"outputs":[],"source":["memory = ConversationBufferMemory()"]},{"cell_type":"code","execution_count":null,"id":"a36e9905","metadata":{"tags":[],"id":"a36e9905"},"outputs":[],"source":["memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})"]},{"cell_type":"code","execution_count":null,"id":"61631b1f","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"61631b1f","executionInfo":{"status":"ok","timestamp":1695946432933,"user_tz":-600,"elapsed":517,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"34cd2d85-d3cb-44a3-a23b-7f2f33e6e790"},"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Hi\n","AI: What's up\n"]}],"source":["print(memory.buffer)"]},{"cell_type":"code","execution_count":null,"id":"a2fdf9ec","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"a2fdf9ec","executionInfo":{"status":"ok","timestamp":1695946436674,"user_tz":-600,"elapsed":320,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"af98089a-4d13-4794-ed2e-ca7f26a896a0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"Human: Hi\\nAI: What's up\"}"]},"metadata":{},"execution_count":13}],"source":["memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":null,"id":"7ca79256","metadata":{"tags":[],"id":"7ca79256"},"outputs":[],"source":["memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})"]},{"cell_type":"code","execution_count":null,"id":"890a4497","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"890a4497","executionInfo":{"status":"ok","timestamp":1695946443948,"user_tz":-600,"elapsed":313,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"e12c8d3d-6f2b-4c74-cfeb-8f89619210e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"]},"metadata":{},"execution_count":15}],"source":["memory.load_memory_variables({})"]},{"cell_type":"markdown","id":"cf98e9ff","metadata":{"tags":[],"id":"cf98e9ff"},"source":["## ConversationBufferWindowMemory"]},{"cell_type":"code","execution_count":null,"id":"66eeccc3","metadata":{"tags":[],"id":"66eeccc3"},"outputs":[],"source":["from langchain.memory import ConversationBufferWindowMemory"]},{"cell_type":"code","execution_count":null,"id":"3ea6233e","metadata":{"tags":[],"id":"3ea6233e"},"outputs":[],"source":["memory = ConversationBufferWindowMemory(k=1)"]},{"cell_type":"code","execution_count":null,"id":"dc4553fb","metadata":{"tags":[],"id":"dc4553fb"},"outputs":[],"source":["memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n"]},{"cell_type":"code","execution_count":null,"id":"6a788403","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"6a788403","executionInfo":{"status":"ok","timestamp":1695946468746,"user_tz":-600,"elapsed":4,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"49c2c1fc-24e7-4a51-ee5a-f6c1e437702a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'Human: Not much, just hanging\\nAI: Cool'}"]},"metadata":{},"execution_count":19}],"source":["memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":null,"id":"4087bc87","metadata":{"tags":[],"id":"4087bc87"},"outputs":[],"source":["llm = ChatOpenAI(temperature=0.0, model=llm_model)\n","memory = ConversationBufferWindowMemory(k=1)\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=False\n",")"]},{"cell_type":"code","execution_count":null,"id":"4faaa952","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"4faaa952","executionInfo":{"status":"ok","timestamp":1695946491267,"user_tz":-600,"elapsed":851,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"86f590c8-7296-477e-de9d-704179343230"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Hello Andrew! It's nice to meet you. How can I assist you today?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["conversation.predict(input=\"Hi, my name is Andrew\")"]},{"cell_type":"code","execution_count":null,"id":"bb20ddaa","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bb20ddaa","executionInfo":{"status":"ok","timestamp":1695946493766,"user_tz":-600,"elapsed":521,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"738aaa95-a9a9-4437-ccd3-de8d93fc5496"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1+1 is equal to 2.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}],"source":["conversation.predict(input=\"What is 1+1?\")"]},{"cell_type":"code","execution_count":null,"id":"489b2194","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"489b2194","executionInfo":{"status":"ok","timestamp":1695946496970,"user_tz":-600,"elapsed":1499,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"38fffc43-368e-4157-d5d9-977836d93075"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}],"source":["conversation.predict(input=\"What is my name?\")"]},{"cell_type":"markdown","id":"d2931b92","metadata":{"id":"d2931b92"},"source":["## ConversationTokenBufferMemory"]},{"cell_type":"code","execution_count":null,"id":"fb9020ed","metadata":{"tags":[],"id":"fb9020ed"},"outputs":[],"source":["from langchain.memory import ConversationTokenBufferMemory\n","from langchain.llms import OpenAI\n","llm = ChatOpenAI(temperature=0.0, model=llm_model)"]},{"cell_type":"code","execution_count":null,"id":"43582ee6","metadata":{"tags":[],"id":"43582ee6"},"outputs":[],"source":["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n","memory.save_context({\"input\": \"AI is what?!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"},\n","                    {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"},\n","                    {\"output\": \"Charming!\"})"]},{"cell_type":"code","execution_count":null,"id":"284288e1","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"284288e1","executionInfo":{"status":"ok","timestamp":1695946563467,"user_tz":-600,"elapsed":4,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"14fead7f-2835-4d82-d7c2-e0f20e5a2816"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"]},"metadata":{},"execution_count":27}],"source":["memory.load_memory_variables({})"]},{"cell_type":"markdown","id":"5ff55d5d","metadata":{"id":"5ff55d5d"},"source":["## ConversationSummaryMemory"]},{"cell_type":"code","execution_count":null,"id":"72dcf8b1","metadata":{"tags":[],"id":"72dcf8b1"},"outputs":[],"source":["from langchain.memory import ConversationSummaryBufferMemory"]},{"cell_type":"code","execution_count":null,"id":"4a5b238f","metadata":{"tags":[],"id":"4a5b238f"},"outputs":[],"source":["# create a long string\n","schedule = \"There is a meeting at 8am with your product team. \\\n","You will need your powerpoint presentation prepared. \\\n","9am-12pm have time to work on your LangChain \\\n","project which will go quickly because Langchain is such a powerful tool. \\\n","At Noon, lunch at the italian resturant with a customer who is driving \\\n","from over an hour away to meet you to understand the latest in AI. \\\n","Be sure to bring your laptop to show the latest LLM demo.\"\n","\n","memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n","memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n","memory.save_context({\"input\": \"What is on the schedule today?\"},\n","                    {\"output\": f\"{schedule}\"})"]},{"cell_type":"code","execution_count":null,"id":"2e4ecabe","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"2e4ecabe","executionInfo":{"status":"ok","timestamp":1695946603180,"user_tz":-600,"elapsed":342,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"7aebadad-205e-424f-e8c6-30c33a65cd86"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.'}"]},"metadata":{},"execution_count":30}],"source":["memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":null,"id":"6728edba","metadata":{"tags":[],"id":"6728edba"},"outputs":[],"source":["conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"9a221b1d","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":303},"id":"9a221b1d","executionInfo":{"status":"ok","timestamp":1695946664907,"user_tz":-600,"elapsed":12176,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"858ea9ca-2f0f-4fe7-8285-4b9978cebbb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.\n","Human: What would be a good demo to show?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'A good demo to show during the lunch meeting with the customer interested in AI would be the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, you can demonstrate the capabilities of our AI technology and how it can be applied to various industries and use cases.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}],"source":["conversation.predict(input=\"What would be a good demo to show?\")"]},{"cell_type":"code","execution_count":null,"id":"bb582617","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"bb582617","executionInfo":{"status":"ok","timestamp":1695946681069,"user_tz":-600,"elapsed":339,"user":{"displayName":"Muhammad Raza","userId":"10265348660845665358"}},"outputId":"1e173093-42ba-4532-8426-4596cc73abd4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting. The human asks what would be a good demo to show, and the AI suggests showcasing the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, the AI believes they can demonstrate the capabilities of their AI technology and how it can be applied to various industries and use cases.'}"]},"metadata":{},"execution_count":33}],"source":["memory.load_memory_variables({})"]},{"cell_type":"markdown","id":"c6709b89-182e-4b7b-b811-1584dda20652","metadata":{"id":"c6709b89-182e-4b7b-b811-1584dda20652"},"source":["Reminder: Download your notebook to you local computer to save your work."]},{"cell_type":"code","execution_count":null,"id":"4ba827aa","metadata":{"id":"4ba827aa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"2c07922b","metadata":{"id":"2c07922b"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"52696c8c","metadata":{"id":"52696c8c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"48690d13","metadata":{"id":"48690d13"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"85bba1f8","metadata":{"id":"85bba1f8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"96a62e89","metadata":{"id":"96a62e89"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f953ad8d","metadata":{"id":"f953ad8d"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b9bb2617","metadata":{"id":"b9bb2617"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"be1989bf","metadata":{"id":"be1989bf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b1f8dca6","metadata":{"id":"b1f8dca6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e9120949","metadata":{"id":"e9120949"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"55849fb5","metadata":{"id":"55849fb5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"94ae491b","metadata":{"id":"94ae491b"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e7c3aec3","metadata":{"id":"e7c3aec3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5ae51a3f","metadata":{"id":"5ae51a3f"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"06894001","metadata":{"id":"06894001"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e2fb674c","metadata":{"id":"e2fb674c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5158f4c8","metadata":{"id":"5158f4c8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"2cebe066","metadata":{"id":"2cebe066"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"65caee4d","metadata":{"id":"65caee4d"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"04799230","metadata":{"id":"04799230"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"30795c10","metadata":{"id":"30795c10"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"08ed59c4","metadata":{"id":"08ed59c4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"bb43eee3","metadata":{"id":"bb43eee3"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}